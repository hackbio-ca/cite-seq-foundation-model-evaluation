{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to do an assessment of mutual information between the CITE-seq ADT (surface protein expression) and cytoplasmic RNA expression (SCT) in the PBMC cells. The MI will be calculated once again by the LatentMI method, but in this case, we'll test the method with different features - including adding scGPT, Orthrus, and ESM2 embeddings to the cell and protein spaces. \n",
    "\n",
    "scGPT features will be added to the cell, and protein spaces, while Orthrus and ESM2 embeddings will be added to the protein space. The embeddings will be added to the protein space by concatenating the embeddings to the protein expression matrix. Similarly, the scGPT embeddings will be added to the cell space by concatenating the embeddings to the SCT matrix.\n",
    "\n",
    "Using the ceiling of the number of features for 2000 HVGs + (scGPT + Orthrus + ESM2 embeddings), we'll also include a baseline with an increased number of HVGs. E.g. if the Orthrus features have the highest number of dimensions, we'll use the top 2000 HVGs + n HVG features (n total orthrus dimensions).\n",
    "\n",
    "Each iteration will be ran 10 times on different random samples of cells in the PBMC cite-seq dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scanpy as sc \n",
    "import anndata as ann \n",
    "import muon as mu\n",
    "import mudata as md \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/hmaan/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/mudata/_core/mudata.py:489: UserWarning: Cannot join columns with the same name because var_names are intersecting.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##Load the MuData object with scgpt, orthrus and esm2 embeddings\n",
    "mdata = md.read(\"/fs01/projects/isoclr/cite_seq_with_seq_embed_with_cell_embed.h5mu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1_AAACCCAAGAAACTCA           CD14 Mono\n",
       "L1_AAACCCAAGACATACA             CD4 TCM\n",
       "L1_AAACCCACAACTGGTT           CD8 Naive\n",
       "L1_AAACCCACACGTACTA                  NK\n",
       "L1_AAACCCACAGCATACT           CD8 Naive\n",
       "                              ...      \n",
       "E2L8_TTTGTTGGTCGTGATT         CD8 Naive\n",
       "E2L8_TTTGTTGGTGTGCCTG         CD14 Mono\n",
       "E2L8_TTTGTTGGTTAGTTCG    B intermediate\n",
       "E2L8_TTTGTTGGTTGGCTAT         CD16 Mono\n",
       "E2L8_TTTGTTGTCTCATGGA         CD14 Mono\n",
       "Name: celltype.l2, Length: 161764, dtype: category\n",
       "Categories (31, object): ['ASDC', 'B intermediate', 'B memory', 'B naive', ..., 'cDC2', 'dnT', 'gdT', 'pDC']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata.obs[\"celltype.l2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all of the scgpt embeddings \n",
    "scgpt_cols = [col for col in mdata[\"SCT\"].obs.columns if \"scgpt\" in col]\n",
    "scgpt_col_indices = [mdata[\"SCT\"].obs.columns.get_loc(col) for col in scgpt_cols]\n",
    "scgpt_embeddings = np.array(mdata[\"SCT\"].obs[scgpt_cols].values)\n",
    "\n",
    "# Extract all of the orthrus embeddings\n",
    "orthrus_cols = [col for col in mdata[\"ADT\"].var.columns if \"orthrus\" in col]\n",
    "orthrus_col_indices = [mdata[\"ADT\"].var.columns.get_loc(col) for col in orthrus_cols]\n",
    "orthrus_embeddings = np.array(mdata[\"ADT\"].var[orthrus_cols].values)\n",
    "\n",
    "# Extract all of the esm2 embeddings\n",
    "esm2_cols = [col for col in mdata[\"ADT\"].var.columns if \"esm\" in col]\n",
    "esm2_col_indices = [mdata[\"ADT\"].var.columns.get_loc(col) for col in esm2_cols]\n",
    "esm2_embeddings = np.array(mdata[\"ADT\"].var[esm2_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within mdata, drop any vars that have nan values for either the esm2 or orthrus scolumns \n",
    "orthrus_cols_subset = mdata[\"ADT\"].var[orthrus_cols]\n",
    "esm2_cols_subset = mdata[\"ADT\"].var[esm2_cols]\n",
    "\n",
    "nan_mask_orthrus = orthrus_cols_subset.isna().any(axis=1)\n",
    "nan_mask_esm2 = esm2_cols_subset.isna().any(axis=1)\n",
    "\n",
    "mdata_adt_subset = mdata[\"ADT\"][:, ~nan_mask_orthrus & ~nan_mask_esm2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same mask, subset the esm2 and orthrus embeddings\n",
    "orthrus_embeddings_subset = orthrus_embeddings[~nan_mask_orthrus & ~nan_mask_esm2]\n",
    "esm2_embeddings_subset = esm2_embeddings[~nan_mask_orthrus & ~nan_mask_esm2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orthrus embeddings shape: (41, 512)\n",
      "esm2 embeddings shape: (41, 320)\n"
     ]
    }
   ],
   "source": [
    "# Determine embedding dimensions\n",
    "print(f\"orthrus embeddings shape: {orthrus_embeddings_subset.shape}\")\n",
    "print(f\"esm2 embeddings shape: {esm2_embeddings_subset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/hmaan/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py:75: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
      "  warnings.warn(\n",
      "/h/hmaan/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:193: RuntimeWarning: invalid value encountered in divide\n",
      "  Xs = torch.from_numpy(np.nan_to_num((Xs - Xs.mean(axis=0)) / Xs.std(axis=0))).float().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200 (of max 300) ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»ðŸŒ»"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     sct_counts_sub \u001b[38;5;241m=\u001b[39m sct_counts[indices, :]\u001b[38;5;241m.\u001b[39mtodense()\n\u001b[1;32m     18\u001b[0m     adt_counts_sub \u001b[38;5;241m=\u001b[39m adt_counts[indices, :]\n\u001b[0;32m---> 19\u001b[0m     pmis, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msct_counts_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madt_counts_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     MI_real[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(pmis)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Save the MI estimate from the calculation and the MI estimates from the simulations\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:210\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(Xs, Ys, regularizer, alpha, lam, N_dims, validation_split, estimate_on_val, batch_size, lr, epochs, patience, quiet, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m indices[:N_train]\n\u001b[1;32m    207\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m indices[N_train:]\n\u001b[0;32m--> 210\u001b[0m Zx, Zy, model \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zx)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zy)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    217\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs in embedding! converted to 0s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:152\u001b[0m, in \u001b[0;36mlearn_representation\u001b[0;34m(Xs, Ys, train_indices, test_indices, regularizer, alpha, lam, N_dims, batch_size, lr, epochs, validation_split, patience, quiet, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# assert X_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# assert Y_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(regularizer)(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], N_dims, \n\u001b[1;32m    150\u001b[0m                           alpha\u001b[38;5;241m=\u001b[39malpha, lam\u001b[38;5;241m=\u001b[39mlam)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:79\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, Y_train, X_test, Y_test, batch_size, lr, epochs, patience, quiet)\u001b[0m\n\u001b[1;32m     76\u001b[0m     model_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearning_loss(X, Y)\n\u001b[1;32m     78\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmodel_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# validation \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start with the HVG baseline \n",
    "mdata_sct_sub = mdata[\"SCT\"]\n",
    "\n",
    "# Get HVG subset of SCT\n",
    "sc.pp.highly_variable_genes(mdata_sct_sub, n_top_genes=2000)\n",
    "sct_counts = mdata[\"SCT\"].X[:, mdata_sct_sub.var.highly_variable]\n",
    "adt_counts = mdata_adt_subset.X\n",
    "\n",
    "# Randomly sample 1000 cells with a seed - repeat 10 times \n",
    "from latentmi import lmi\n",
    "\n",
    "n_mi_calc = 10\n",
    "MI_real = np.zeros(n_mi_calc)\n",
    "for i in range(n_mi_calc):\n",
    "    np.random.seed(i)\n",
    "    indices = np.random.choice(mdata[\"SCT\"].shape[0], 1000, replace=True)\n",
    "    sct_counts_sub = sct_counts[indices, :].todense()\n",
    "    adt_counts_sub = adt_counts[indices, :]\n",
    "    pmis, _, _ = lmi.estimate(sct_counts_sub, adt_counts_sub)\n",
    "    MI_real[i] = np.nanmean(pmis)\n",
    "    \n",
    "# Save the MI estimate from the calculation and the MI estimates from the simulations\n",
    "np.save(\"../data/MI_estimate_1k_10_times_adt_sct_exp_5.npy\", MI_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/hmaan/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:193: RuntimeWarning: invalid value encountered in divide\n",
      "  Xs = torch.from_numpy(np.nan_to_num((Xs - Xs.mean(axis=0)) / Xs.std(axis=0))).float().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 (of max 300) "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Dot product of adt_counts_sub and esm2_embeddings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     adt_esm_product \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(adt_counts_sub, esm2_embeddings_subset)\n\u001b[0;32m---> 13\u001b[0m     pmis, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msct_counts_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madt_esm_product\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     MI_real_esm2[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(pmis)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save the MI estimate from the calculation and the MI estimates from the simulations\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:210\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(Xs, Ys, regularizer, alpha, lam, N_dims, validation_split, estimate_on_val, batch_size, lr, epochs, patience, quiet, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m indices[:N_train]\n\u001b[1;32m    207\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m indices[N_train:]\n\u001b[0;32m--> 210\u001b[0m Zx, Zy, model \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zx)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zy)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    217\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs in embedding! converted to 0s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:152\u001b[0m, in \u001b[0;36mlearn_representation\u001b[0;34m(Xs, Ys, train_indices, test_indices, regularizer, alpha, lam, N_dims, batch_size, lr, epochs, validation_split, patience, quiet, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# assert X_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# assert Y_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(regularizer)(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], N_dims, \n\u001b[1;32m    150\u001b[0m                           alpha\u001b[38;5;241m=\u001b[39malpha, lam\u001b[38;5;241m=\u001b[39mlam)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:79\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, Y_train, X_test, Y_test, batch_size, lr, epochs, patience, quiet)\u001b[0m\n\u001b[1;32m     76\u001b[0m     model_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearning_loss(X, Y)\n\u001b[1;32m     78\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmodel_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# validation \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now test ESM2 - get 2000 HVGs and test by appending the esm2 embeddings to the ADT data\n",
    "\n",
    "# Randomly sample 1000 cells with a seed - repeat 10 times\n",
    "\n",
    "MI_real_esm2 = np.zeros(n_mi_calc)\n",
    "for i in range(n_mi_calc):\n",
    "    np.random.seed(i)\n",
    "    indices = np.random.choice(mdata[\"SCT\"].shape[0], 1000, replace=True)\n",
    "    sct_counts_sub = sct_counts[indices, :].todense()\n",
    "    adt_counts_sub = adt_counts[indices, :]\n",
    "    # Dot product of adt_counts_sub and esm2_embeddings\n",
    "    adt_esm_product = np.dot(adt_counts_sub, esm2_embeddings_subset)\n",
    "    pmis, _, _ = lmi.estimate(sct_counts_sub, adt_esm_product)\n",
    "    MI_real_esm2[i] = np.nanmean(pmis)\n",
    "    \n",
    "# Save the MI estimate from the calculation and the MI estimates from the simulations\n",
    "np.save(\"../data/MI_estimate_1k_10_times_adt_sct_exp_5_esm2.npy\", MI_real_esm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77 (of max 300) ðŸŒ»ðŸŒ»"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Dot product of adt_counts_sub and orthrus_embeddings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     adt_orthrus_product \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(adt_counts_sub, orthrus_embeddings_subset)\n\u001b[0;32m---> 12\u001b[0m     pmis, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msct_counts_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madt_orthrus_product\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     MI_real_orthrus[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(pmis)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save the MI estimate from the calculation and the MI estimates from the simulations\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:210\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(Xs, Ys, regularizer, alpha, lam, N_dims, validation_split, estimate_on_val, batch_size, lr, epochs, patience, quiet, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m indices[:N_train]\n\u001b[1;32m    207\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m indices[N_train:]\n\u001b[0;32m--> 210\u001b[0m Zx, Zy, model \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zx)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zy)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    217\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs in embedding! converted to 0s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:152\u001b[0m, in \u001b[0;36mlearn_representation\u001b[0;34m(Xs, Ys, train_indices, test_indices, regularizer, alpha, lam, N_dims, batch_size, lr, epochs, validation_split, patience, quiet, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# assert X_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# assert Y_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(regularizer)(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], N_dims, \n\u001b[1;32m    150\u001b[0m                           alpha\u001b[38;5;241m=\u001b[39malpha, lam\u001b[38;5;241m=\u001b[39mlam)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:79\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, Y_train, X_test, Y_test, batch_size, lr, epochs, patience, quiet)\u001b[0m\n\u001b[1;32m     76\u001b[0m     model_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearning_loss(X, Y)\n\u001b[1;32m     78\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmodel_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# validation \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now test Orthrus - get 2000 HVGs and test by appending the orthrus embeddings to the ADT data\n",
    "\n",
    "# Randomly sample 1000 cells with a seed - repeat 10 times\n",
    "MI_real_orthrus = np.zeros(n_mi_calc)\n",
    "for i in range(n_mi_calc):\n",
    "    np.random.seed(i)\n",
    "    indices = np.random.choice(mdata[\"SCT\"].shape[0], 1000, replace=True)\n",
    "    sct_counts_sub = sct_counts[indices, :].todense()\n",
    "    adt_counts_sub = adt_counts[indices, :]\n",
    "    # Dot product of adt_counts_sub and orthrus_embeddings\n",
    "    adt_orthrus_product = np.dot(adt_counts_sub, orthrus_embeddings_subset)\n",
    "    pmis, _, _ = lmi.estimate(sct_counts_sub, adt_orthrus_product)\n",
    "    MI_real_orthrus[i] = np.nanmean(pmis)\n",
    "    \n",
    "# Save the MI estimate from the calculation and the MI estimates from the simulations\n",
    "np.save(\"../data/MI_estimate_1k_10_times_adt_sct_exp_5_orthrus.npy\", MI_real_orthrus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 (of max 300) "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     sct_counts_sub \u001b[38;5;241m=\u001b[39m sct_counts_scgpt[indices, :]\n\u001b[1;32m     11\u001b[0m     adt_counts_sub \u001b[38;5;241m=\u001b[39m adt_counts[indices, :]\n\u001b[0;32m---> 12\u001b[0m     pmis, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msct_counts_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madt_counts_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     MI_real_scgpt[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(pmis)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save the MI estimate from the calculation and the MI estimates from the simulations\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:210\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(Xs, Ys, regularizer, alpha, lam, N_dims, validation_split, estimate_on_val, batch_size, lr, epochs, patience, quiet, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m indices[:N_train]\n\u001b[1;32m    207\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m indices[N_train:]\n\u001b[0;32m--> 210\u001b[0m Zx, Zy, model \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zx)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(Zy)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    217\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs in embedding! converted to 0s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:152\u001b[0m, in \u001b[0;36mlearn_representation\u001b[0;34m(Xs, Ys, train_indices, test_indices, regularizer, alpha, lam, N_dims, batch_size, lr, epochs, validation_split, patience, quiet, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# assert X_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# assert Y_train.shape[1] // 4 > 0, \"Hidden layer with size 0. Consider tiling input.\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(regularizer)(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], N_dims, \n\u001b[1;32m    150\u001b[0m                           alpha\u001b[38;5;241m=\u001b[39malpha, lam\u001b[38;5;241m=\u001b[39mlam)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/latentmi/lmi.py:79\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, Y_train, X_test, Y_test, batch_size, lr, epochs, patience, quiet)\u001b[0m\n\u001b[1;32m     76\u001b[0m     model_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearning_loss(X, Y)\n\u001b[1;32m     78\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mmodel_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# validation \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/citeseq_env/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now test SCGPT - substitute the hvg counts for the scgpt embeddings\n",
    "sct_counts_scgpt = scgpt_embeddings\n",
    "adt_counts = mdata_adt_subset.X\n",
    "\n",
    "# Randomly sample 1000 cells with a seed - repeat 10 times\n",
    "MI_real_scgpt = np.zeros(n_mi_calc)\n",
    "for i in range(n_mi_calc):\n",
    "    np.random.seed(i)\n",
    "    indices = np.random.choice(mdata[\"SCT\"].shape[0], 1000, replace=True)\n",
    "    sct_counts_sub = sct_counts_scgpt[indices, :]\n",
    "    adt_counts_sub = adt_counts[indices, :]\n",
    "    pmis, _, _ = lmi.estimate(sct_counts_sub, adt_counts_sub)\n",
    "    MI_real_scgpt[i] = np.nanmean(pmis)\n",
    "\n",
    "# Save the MI estimate from the calculation and the MI estimates from the simulations\n",
    "np.save(\"../data/MI_estimate_1k_10_times_adt_sct_exp_5_scgpt.npy\", MI_real_scgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test including the scgpt and hvg counts together\n",
    "sct_counts_scgpt = np.concatenate((scgpt_embeddings, sct_counts), axis=1) \n",
    "adt_counts = mdata_adt_subset.X\n",
    "\n",
    "# Randomly sample 1000 cells with a seed - repeat 10 times\n",
    "MI_real_scgpt_hvg = np.zeros(n_mi_calc)\n",
    "for i in range(n_mi_calc):\n",
    "    np.random.seed(i)\n",
    "    indices = np.random.choice(mdata[\"SCT\"].shape[0], 1000, replace=True)\n",
    "    sct_counts_sub = sct_counts_scgpt[indices, :]\n",
    "    adt_counts_sub = adt_counts[indices, :]\n",
    "    pmis, _, _ = lmi.estimate(sct_counts_sub, adt_counts_sub)\n",
    "    MI_real_scgpt_hvg[i] = np.nanmean(pmis)\n",
    "    \n",
    "# Save the MI estimate from the calculation and the MI estimates from the simulations\n",
    "np.save(\"../data/MI_estimate_1k_10_times_adt_sct_exp_5_scgpt_hvg.npy\", MI_real_scgpt_hvg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeseq_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
